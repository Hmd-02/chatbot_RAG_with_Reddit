{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Teknoloogiaa*: Chatbot Basé sur GPT-2\n",
    "\n",
    "Ce notebook présente une démarche complète pour créer un chatbot en anglais à partir du modèle pré-entraîné GPT-2. Il couvre l’ensemble du processus, du traitement des données jusqu’à l’interface utilisateur.\n",
    "\n",
    "### Objectifs\n",
    "\n",
    "- Charger et préparer les données textuelles pour l’entraînement.\n",
    "- Affiner le modèle GPT-2 avec des données personnalisées.\n",
    "- Développer une fonction de réponse automatique basée sur le modèle.\n",
    "- Intégrer le chatbot dans une interface interactive avec *Gradio*.\n",
    "\n",
    "### Données et source:\n",
    "- Les données utilisées comme corpus pour le Chatbot sont des commentaires issus de posts sur Reddit. Ces posts sont en rapport avec la technologie (l'Intelligence Artificielle (IA), le Machine Learning..)\n",
    "\n",
    "### Étapes principales\n",
    "\n",
    "1. *Installation des bibliothèques nécessaires* (Transformers, Gradio, etc.)\n",
    "2. *Chargement et prétraitement des fichiers textes*\n",
    "3. *Fine-tuning* du modèle GPT-2 avec gestion des ressources GPU\n",
    "4. *Création d’une fonction de génération de réponses*\n",
    "5. *Évaluation du chatbot*\n",
    "6. *Déploiement de l'interface Gradio pour tester le chatbot*\n",
    "\n",
    "### Dépendances\n",
    "\n",
    "- transformers : pour charger, configurer et entraîner le modèle GPT-2.\n",
    "- torch : la bibliothèque PyTorch utilisée pour entraîner et manipuler les modèles de deep learning.\n",
    "- pandas : pour charger et manipuler les fichiers de données textuelles au format tabulaire.\n",
    "- gradio : pour créer une interface web interactive et tester le chatbot en direct.\n",
    "- sklearn : pour certaines fonctions utilitaires comme la séparation des jeux de données.\n",
    "- numpy : pour les opérations numériques et la gestion efficace des tableaux de données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation et Importation des packages et des bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-23T23:31:43.896481Z",
     "iopub.status.busy": "2025-04-23T23:31:43.896182Z",
     "iopub.status.idle": "2025-04-23T23:31:47.115341Z",
     "shell.execute_reply": "2025-04-23T23:31:47.114484Z",
     "shell.execute_reply.started": "2025-04-23T23:31:43.896461Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.26.0)\n",
      "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.9.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.9.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.6)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (14.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install gradio hf_xet\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import glob\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Téléchargement et fine-tuning du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:32:03.673536Z",
     "iopub.status.busy": "2025-04-23T23:32:03.673217Z",
     "iopub.status.idle": "2025-04-23T23:32:03.699389Z",
     "shell.execute_reply": "2025-04-23T23:32:03.698871Z",
     "shell.execute_reply.started": "2025-04-23T23:32:03.673498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# SECTION 1: VÉRIFICATION DU MATÉRIEL ET UTILITAIRES DE BASE\n",
    "# ===============================================================\n",
    "\n",
    "# Vérifier la disponibilité du GPU\n",
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # En GB\n",
    "        print(f\"GPU disponible: {gpu_name} ({gpu_memory:.2f} GB)\")\n",
    "        return device\n",
    "    else:\n",
    "        print(\"Aucun GPU détecté, utilisation du CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 2: CHARGEMENT ET PRÉPARATION DES DONNÉES\n",
    "# ===============================================================\n",
    "\n",
    "# Classe pour charger les documents txt depuis notre repertoire\n",
    "class SimpleDirectoryReader:\n",
    "    def __init__(self, directory_path):\n",
    "        self.directory_path = directory_path\n",
    "        \n",
    "    def load_data(self):\n",
    "        documents = []\n",
    "        for file_path in glob.glob(os.path.join(self.directory_path, \"*.txt\")):\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                documents.append(Document(text, extra_info={\"source\": file_path}))\n",
    "        return documents\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, text, extra_info=None):\n",
    "        self.text = text\n",
    "        self.extra_info = extra_info or {}\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 3: TÉLÉCHARGEMENT ET PRÉPARATION DU MODÈLE GPT-2\n",
    "# ===============================================================\n",
    "\n",
    "# Téléchargeons le modèle GPT-2 Medium et le tokenizer\n",
    "def download_and_save_model():\n",
    "    model_dir = \"/kaggle/working/gpt2-medium\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Télécharger le tokenizer et le modèle\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "    # Définir le token de padding pour GPT-2\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "    # Mettre à jour la configuration du modèle pour reconnaître le pad_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    # Sauvegarder le modèle et le tokenizer\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    model.save_pretrained(model_dir)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Afficher un message de confirmation\n",
    "    print(f\"Temps écoulé: {elapsed_time:.2f} secondes\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 4: PRÉPARATION DES DONNÉES POUR L'ENTRAÎNEMENT\n",
    "# ===============================================================\n",
    "\n",
    "# Classe de dataset personnalisée\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Fonction pour diviser les textes en chunks\n",
    "def chunk_text(text, chunk_size_limit=600, overlap=20):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size_limit - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size_limit])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 5: CONSTRUCTION DE L'INDEX POUR LA RECHERCHE SÉMANTIQUE\n",
    "# ===============================================================\n",
    "\n",
    "# Construction de l'index\n",
    "def construct_index(directory_path):\n",
    "    # Vérifier le GPU\n",
    "    device = check_gpu()\n",
    "    \n",
    "    # Paramètres\n",
    "    max_input_size = 512  # Limité pour GPT-2\n",
    "    num_outputs = 256\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 600\n",
    "    \n",
    "    # Télécharger et sauvegarder le modèle\n",
    "    tokenizer, model = download_and_save_model()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Vérifier si des documents existent dans le répertoire\n",
    "    document_files = glob.glob(os.path.join(directory_path, \"*.txt\"))\n",
    "    if not document_files:\n",
    "        print(f\"Aucun fichier .txt trouvé dans {directory_path}. Création d'un exemple simple...\")\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    \n",
    "    if not documents:\n",
    "        raise ValueError(f\"Aucun document n'a été chargé depuis {directory_path}\")\n",
    "    \n",
    "    print(f\"{len(documents)} document(s) chargé(s)\")\n",
    "    \n",
    "    # Prétraiter les documents\n",
    "    all_chunks = []\n",
    "    document_embeddings = []\n",
    "    print(f\"Création des chunks et des embeddings...\")\n",
    "\n",
    "    \n",
    "    # Barre de progression pour les chunks afin de suivre le traitement\n",
    "    for doc_idx, doc in enumerate(tqdm(documents, desc=\"Documents\", position=0)):\n",
    "        chunks = chunk_text(doc.text, chunk_size_limit, max_chunk_overlap)\n",
    "        all_chunks.extend(chunks)\n",
    "        \n",
    "        # Créer des embeddings simples avec le modèle GPT-2\n",
    "        for chunk_idx, chunk in enumerate(tqdm(chunks, desc=f\"Embeddings pour doc {doc_idx+1}/{len(documents)}\", position=1, leave=False)):\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_size).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "                # Utiliser la moyenne de la dernière couche cachée comme embedding\n",
    "                last_hidden_state = outputs.hidden_states[-1].mean(dim=1)\n",
    "                document_embeddings.append(last_hidden_state.squeeze().cpu().numpy())\n",
    "    \n",
    "    # Convertir en array numpy pour faciliter la récupération\n",
    "    document_embeddings = np.array(document_embeddings)\n",
    "    \n",
    "    # Sauvegarder les données nécessaires\n",
    "    index_data = {\n",
    "        \"chunks\": all_chunks,\n",
    "        \"embeddings\": document_embeddings,\n",
    "    }\n",
    "    \n",
    "    torch.save(index_data, \"/kaggle/working/index.pt\")\n",
    "    \n",
    "    print(f\"Index créé et sauvegardé dans /kaggle/working/index.pt\")\n",
    "    \n",
    "    return index_data\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 6: FINE-TUNING DU MODÈLE SUR LES DOCUMENTS\n",
    "# ===============================================================\n",
    "\n",
    "# Fonction pour fine-tuner le modèle sur les documents\n",
    "def fine_tune_model(directory_path, epochs=3, batch_size=4, learning_rate=5e-5):\n",
    "    # Vérifier le GPU\n",
    "    device = check_gpu()\n",
    "    \n",
    "    # Télécharger le modèle\n",
    "    tokenizer, model = download_and_save_model()\n",
    "    \n",
    "    # Charger les documents\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    texts = [doc.text for doc in documents]\n",
    "    \n",
    "    if not texts:\n",
    "        raise ValueError(f\"Aucun document trouvé pour le fine-tuning dans {directory_path}\")\n",
    "    \n",
    "    print(f\"{len(texts)} texte(s) chargé(s) pour l'entraînement\")\n",
    "    \n",
    "    # Créer le dataset\n",
    "    dataset = TextDataset(texts, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Configurer l'optimiseur\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Déplacer le modèle sur GPU \n",
    "    model.to(device)\n",
    "    \n",
    "    # Boucle d'entraînement\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Initialiser le meilleur loss pour sauvegarder le meilleur modèle\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n🔄 Epoch {epoch+1}/{epochs}\")\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Training\", position=0, \n",
    "                           bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}')\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            # Obtenir les entrées et les envoyer sur GPU\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Réinitialiser les gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Calculer les métriques et mettre à jour la barre de progression\n",
    "            avg_loss = total_loss / (step + 1)\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            time_per_step = elapsed / (step + 1)\n",
    "            remaining_steps = len(dataloader) - (step + 1)\n",
    "            eta = time_per_step * remaining_steps\n",
    "            \n",
    "            # Mettre à jour la barre de progression avec des informations détaillées\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'elapsed': f'{int(elapsed//60)}m {int(elapsed%60)}s',\n",
    "                'ETA': f'{int(eta//60)}m {int(eta%60)}s',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "            \n",
    "            # Libérer la mémoire\n",
    "            if step % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculer la perte moyenne pour cette époque\n",
    "        epoch_avg_loss = total_loss / len(dataloader)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} terminée: Loss = {epoch_avg_loss:.4f}, Temps = {int(epoch_time//60)}m {int(epoch_time%60)}s\")\n",
    "        \n",
    "        # Sauvegarder le meilleur modèle\n",
    "        if epoch_avg_loss < best_loss:\n",
    "            best_loss = epoch_avg_loss\n",
    "            best_model_dir = \"/kaggle/working/gpt2-medium-fine-tuned-best\"\n",
    "            os.makedirs(best_model_dir, exist_ok=True)\n",
    "            model.save_pretrained(best_model_dir)\n",
    "            tokenizer.save_pretrained(best_model_dir)\n",
    "            print(f\"Meilleur modèle sauvegardé (loss: {best_loss:.4f})\")\n",
    "    \n",
    "    # Sauvegarder le modèle fine-tuné final\n",
    "    fine_tuned_dir = \"/kaggle/working/gpt2-medium-fine-tuned\"\n",
    "    os.makedirs(fine_tuned_dir, exist_ok=True)\n",
    "    model.save_pretrained(fine_tuned_dir)\n",
    "    tokenizer.save_pretrained(fine_tuned_dir)\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"padding: 10px; border-radius: 5px; background-color: #eafaf1; border-left: 5px solid #2ecc71;\">\n",
    "      <h3 style=\"margin: 0;\">✅ Fine-tuning terminé avec succès!</h3>\n",
    "      <p>Modèle final sauvegardé dans <code>{fine_tuned_dir}</code></p>\n",
    "      <p>Meilleur modèle sauvegardé dans <code>/kaggle/working/gpt2-medium-fine-tuned-best</code> (loss: {best_loss:.4f})</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de la fonction pour poser les questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T00:50:04.085851Z",
     "iopub.status.busy": "2025-04-24T00:50:04.085198Z",
     "iopub.status.idle": "2025-04-24T00:50:04.095543Z",
     "shell.execute_reply": "2025-04-24T00:50:04.094715Z",
     "shell.execute_reply.started": "2025-04-24T00:50:04.085826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fonction pour poser des questions\n",
    "def ask_me_anything(question, use_fine_tuned=True):\n",
    "    # Charger le modèle approprié\n",
    "    if use_fine_tuned and os.path.exists(\"/kaggle/input/base-de-reddit/modele-pre-entraine/modele-pre-entraine\"):\n",
    "        model_path = \"/kaggle/input/base-de-reddit/modele-pre-entraine/modele-pre-entraine\"\n",
    "    else:\n",
    "        model_path = \"/kaggle/working/gpt2-medium\"\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    # Définir le token de padding\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    # Charger l'index\n",
    "    index_data = torch.load(\"/kaggle/input/base-de-reddit/index.pt\")\n",
    "    chunks = index_data[\"chunks\"]\n",
    "    embeddings = index_data[\"embeddings\"]\n",
    "    \n",
    "    # Créer un embedding pour la question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        question_embedding = outputs.hidden_states[-1].mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    # Calculer les similarités avec les chunks\n",
    "    similarities = cosine_similarity([question_embedding], embeddings)[0]\n",
    "    \n",
    "    # Trouver les chunks les plus pertinents\n",
    "    top_idx = np.argsort(similarities)[-3:][::-1]  # Top 3 chunks les plus similaires\n",
    "    \n",
    "    # CORRECTION: Limiter la taille du contexte pour ne pas dépasser max_length\n",
    "    context_chunks = [chunks[i] for i in top_idx]\n",
    "    \n",
    "    # Choisir un contexte plus court si nécessaire\n",
    "    prompt = f\"Question: {question}\\nContexte: {context_chunks[0]}\\nRéponse:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Si le prompt est encore trop long, utiliser uniquement la question\n",
    "    if inputs[\"input_ids\"].shape[1] > 400:  # Laisse de la marge pour la génération\n",
    "        prompt = f\"Question: {question}\\nRéponse:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    response = response.split(\"Réponse:\")[-1].strip()\n",
    "    \n",
    "    display(Markdown(f\"You asked: <b>{question}</b>\"))\n",
    "    display(Markdown(f\"Bot says: <b>{response}</b>\"))\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle sur nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-23T22:32:22.857846Z",
     "iopub.status.busy": "2025-04-23T22:32:22.857592Z",
     "iopub.status.idle": "2025-04-23T22:33:45.991167Z",
     "shell.execute_reply": "2025-04-23T22:33:45.990556Z",
     "shell.execute_reply.started": "2025-04-23T22:32:22.857818Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: Tesla T4 (15.83 GB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27e3494a8b84ad681d67d6b65f9882d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d967a729b145a9b33b9311b8b15f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40b1cdde7204750ad3f6d0ee4adb886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6811089fed4a70b1346f498c69dc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad01e1ead1d540a39eba4c9bcb797439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093058ac7d9847ec9603a1be9dc57273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a066b4f2344f8f9b1f2517368ec505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps écoulé: 10.85 secondes\n",
      "1 texte(s) chargé(s) pour l'entraînement\n",
      "\n",
      "🔄 Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7e32c4a5ed4ecab4ee65f88ce39815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 terminée: Loss = 3.7245, Temps = 0m 1s\n",
      "Meilleur modèle sauvegardé (loss: 3.7245)\n",
      "\n",
      "🔄 Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b84ea79ff9430cb292152d94f20fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 terminée: Loss = 3.2840, Temps = 0m 0s\n",
      "Meilleur modèle sauvegardé (loss: 3.2840)\n",
      "\n",
      "🔄 Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9a9b9a6a7a4fb89e31a0f8a3d8633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 terminée: Loss = 2.9781, Temps = 0m 0s\n",
      "Meilleur modèle sauvegardé (loss: 2.9781)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"padding: 10px; border-radius: 5px; background-color: #eafaf1; border-left: 5px solid #2ecc71;\">\n",
       "      <h3 style=\"margin: 0;\">✅ Fine-tuning terminé avec succès!</h3>\n",
       "      <p>Modèle final sauvegardé dans <code>/kaggle/working/gpt2-medium-fine-tuned</code></p>\n",
       "      <p>Meilleur modèle sauvegardé dans <code>/kaggle/working/gpt2-medium-fine-tuned-best</code> (loss: 2.9781)</p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Construire l'index\n",
    "#index_data = construct_index(\"/kaggle/input/base-de-reddit/\")\n",
    "\n",
    "# 2. Fine-tuner le modèle\n",
    "#model, tokenizer = fine_tune_model(\"/kaggle/input/base-de-reddit\", epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester le chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T00:50:11.875871Z",
     "iopub.status.busy": "2025-04-24T00:50:11.875615Z",
     "iopub.status.idle": "2025-04-24T00:50:29.454455Z",
     "shell.execute_reply": "2025-04-24T00:50:29.453750Z",
     "shell.execute_reply.started": "2025-04-24T00:50:11.875856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1451110233.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  index_data = torch.load(\"/kaggle/input/base-de-reddit/index.pt\")\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You asked: <b>What do people think about AI ?</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Bot says: <b>We are getting to the point where AI is not only going to be pervasive, it's going to be dominant. I would argue that by 2030, AI will have become a dominant force in our economy.\n",
       "AI will be the norm and you'll see it everywhere. You'll see people doing things in their home, your car, your home office, your job, your home office. You'll see it everywhere.\n",
       "AI is going to be ubiquitous and its impact will be felt across all sectors of our economy.\n",
       "And by 2030, you'll see AI becoming more and more of a driver of economic growth.\n",
       "The point is that AI is going to be a dominant force in the future.\n",
       "AI is going to be ubiquitous and its impact will be felt across all sectors of our economy.\n",
       "If AI is going to be pervasive, it's going to be dominant in all sectors.\n",
       "If AI is going to be dominant in all sectors, it's going to be dominant in every</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"We are getting to the point where AI is not only going to be pervasive, it's going to be dominant. I would argue that by 2030, AI will have become a dominant force in our economy.\\nAI will be the norm and you'll see it everywhere. You'll see people doing things in their home, your car, your home office, your job, your home office. You'll see it everywhere.\\nAI is going to be ubiquitous and its impact will be felt across all sectors of our economy.\\nAnd by 2030, you'll see AI becoming more and more of a driver of economic growth.\\nThe point is that AI is going to be a dominant force in the future.\\nAI is going to be ubiquitous and its impact will be felt across all sectors of our economy.\\nIf AI is going to be pervasive, it's going to be dominant in all sectors.\\nIf AI is going to be dominant in all sectors, it's going to be dominant in every\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_me_anything(\"What do people think about AI ?\", use_fine_tuned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T22:46:45.305798Z",
     "iopub.status.busy": "2025-04-23T22:46:45.305113Z",
     "iopub.status.idle": "2025-04-23T22:47:58.559302Z",
     "shell.execute_reply": "2025-04-23T22:47:58.558357Z",
     "shell.execute_reply.started": "2025-04-23T22:46:45.305770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/ (stored 0%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/vocab.json (deflated 68%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/special_tokens_map.json (deflated 74%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/config.json (deflated 53%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/generation_config.json (deflated 24%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/merges.txt (deflated 53%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/model.safetensors (deflated 7%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/tokenizer_config.json (deflated 56%)\n"
     ]
    }
   ],
   "source": [
    "#!zip -r /kaggle/working/gpt2-medium-fine-tuned-best.zip /kaggle/working/gpt2-medium-fine-tuned-best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de l'interface gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:34:43.895434Z",
     "iopub.status.busy": "2025-04-23T23:34:43.895153Z",
     "iopub.status.idle": "2025-04-23T23:34:48.917229Z",
     "shell.execute_reply": "2025-04-23T23:34:48.916504Z",
     "shell.execute_reply.started": "2025-04-23T23:34:43.895414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3760614080.py:31: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "* Running on public URL: https://e238daadaf84a1a7e8.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e238daadaf84a1a7e8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Définir la variable device si elle n'est pas déjà définie\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Fonction pour interagir avec le chatbot via Gradio\n",
    "def gradio_ask(question, history):\n",
    "    # Chargement des modèles et des données\n",
    "    if not os.path.exists(\"/kaggle/working/index.pt\"):\n",
    "        return \"Erreur: Veuillez d'abord exécuter construct_index() pour créer l'index.\", history\n",
    "    \n",
    "    # Vous pouvez modifier ce paramètre pour utiliser le modèle fine-tuné ou non\n",
    "    use_fine_tuned = os.path.exists(\"/kaggle/working/gpt2-medium-fine-tuned\")\n",
    "    \n",
    "    # Utiliser la fonction existante\n",
    "    response = ask_me_anything(question, use_fine_tuned=use_fine_tuned)\n",
    "    \n",
    "    # Formater pour Gradio - histoire normale, pas de type 'messages'\n",
    "    history.append((question, response))\n",
    "    return \"\", history\n",
    "\n",
    "# Création de l'interface Gradio\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks(title=\"Teknoloogiaa\") as demo:\n",
    "        gr.Markdown(\"# Teknoloogiaa\")\n",
    "        gr.Markdown(\"Posez vos questions a Teknoloogiaa sur tout ce qui est en rapport avec l'IA, le machine learning, la datascience, ...\")\n",
    "        \n",
    "        # Retirez le paramètre type='messages'\n",
    "        chatbot = gr.Chatbot(height=300)\n",
    "        \n",
    "        # Création d'une ligne avec un champ texte et un bouton d'envoi\n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(placeholder=\"Posez votre question ici...\", lines=1, scale=4)\n",
    "            submit_btn = gr.Button(\"Envoyer\", scale=1)\n",
    "        \n",
    "        clear = gr.Button(\"Effacer la conversation\")\n",
    "        \n",
    "        # Connecter le bouton d'envoi à la fonction gradio_ask\n",
    "        submit_btn.click(gradio_ask, [msg, chatbot], [msg, chatbot])\n",
    "        \n",
    "        # Garder la possibilité d'envoyer en appuyant sur Entrée\n",
    "        msg.submit(gradio_ask, [msg, chatbot], [msg, chatbot])\n",
    "        \n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "        \n",
    "        \n",
    "    return demo\n",
    "\n",
    "# Pour lancer l'interface:\n",
    "demo = create_gradio_interface()\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7225506,
     "sourceId": 11536596,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
