{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Teknoloogiaa*: Chatbot Bas√© sur GPT-2\n",
    "\n",
    "Ce notebook pr√©sente une d√©marche compl√®te pour cr√©er un chatbot en anglais √† partir du mod√®le pr√©-entra√Æn√© GPT-2. Il couvre l‚Äôensemble du processus, du traitement des donn√©es jusqu‚Äô√† l‚Äôinterface utilisateur.\n",
    "\n",
    "### Objectifs\n",
    "\n",
    "- Charger et pr√©parer les donn√©es textuelles pour l‚Äôentra√Ænement.\n",
    "- Affiner le mod√®le GPT-2 avec des donn√©es personnalis√©es.\n",
    "- D√©velopper une fonction de r√©ponse automatique bas√©e sur le mod√®le.\n",
    "- Int√©grer le chatbot dans une interface interactive avec *Gradio*.\n",
    "\n",
    "### Donn√©es et source:\n",
    "- Les donn√©es utilis√©es comme corpus pour le Chatbot sont des commentaires issus de posts sur Reddit. Ces posts sont en rapport avec la technologie (l'Intelligence Artificielle (IA), le Machine Learning..)\n",
    "\n",
    "### √âtapes principales\n",
    "\n",
    "1. *Installation des biblioth√®ques n√©cessaires* (Transformers, Gradio, etc.)\n",
    "2. *Chargement et pr√©traitement des fichiers textes*\n",
    "3. *Fine-tuning* du mod√®le GPT-2 avec gestion des ressources GPU\n",
    "4. *Cr√©ation d‚Äôune fonction de g√©n√©ration de r√©ponses*\n",
    "5. *√âvaluation du chatbot*\n",
    "6. *D√©ploiement de l'interface Gradio pour tester le chatbot*\n",
    "\n",
    "### D√©pendances\n",
    "\n",
    "- transformers : pour charger, configurer et entra√Æner le mod√®le GPT-2.\n",
    "- torch : la biblioth√®que PyTorch utilis√©e pour entra√Æner et manipuler les mod√®les de deep learning.\n",
    "- pandas : pour charger et manipuler les fichiers de donn√©es textuelles au format tabulaire.\n",
    "- gradio : pour cr√©er une interface web interactive et tester le chatbot en direct.\n",
    "- sklearn : pour certaines fonctions utilitaires comme la s√©paration des jeux de donn√©es.\n",
    "- numpy : pour les op√©rations num√©riques et la gestion efficace des tableaux de donn√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation et Importation des packages et des bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-23T23:31:43.896481Z",
     "iopub.status.busy": "2025-04-23T23:31:43.896182Z",
     "iopub.status.idle": "2025-04-23T23:31:47.115341Z",
     "shell.execute_reply": "2025-04-23T23:31:47.114484Z",
     "shell.execute_reply.started": "2025-04-23T23:31:43.896461Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.26.0)\n",
      "Requirement already satisfied: hf_xet in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.9.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.9.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.6)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.0->gradio) (14.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install gradio hf_xet\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import glob\n",
    "from IPython.display import Markdown, display, HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T√©l√©chargement et fine-tuning du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:32:03.673536Z",
     "iopub.status.busy": "2025-04-23T23:32:03.673217Z",
     "iopub.status.idle": "2025-04-23T23:32:03.699389Z",
     "shell.execute_reply": "2025-04-23T23:32:03.698871Z",
     "shell.execute_reply.started": "2025-04-23T23:32:03.673498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# SECTION 1: V√âRIFICATION DU MAT√âRIEL ET UTILITAIRES DE BASE\n",
    "# ===============================================================\n",
    "\n",
    "# V√©rifier la disponibilit√© du GPU\n",
    "def check_gpu():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # En GB\n",
    "        print(f\"GPU disponible: {gpu_name} ({gpu_memory:.2f} GB)\")\n",
    "        return device\n",
    "    else:\n",
    "        print(\"Aucun GPU d√©tect√©, utilisation du CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 2: CHARGEMENT ET PR√âPARATION DES DONN√âES\n",
    "# ===============================================================\n",
    "\n",
    "# Classe pour charger les documents txt depuis notre repertoire\n",
    "class SimpleDirectoryReader:\n",
    "    def __init__(self, directory_path):\n",
    "        self.directory_path = directory_path\n",
    "        \n",
    "    def load_data(self):\n",
    "        documents = []\n",
    "        for file_path in glob.glob(os.path.join(self.directory_path, \"*.txt\")):\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                text = file.read()\n",
    "                documents.append(Document(text, extra_info={\"source\": file_path}))\n",
    "        return documents\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, text, extra_info=None):\n",
    "        self.text = text\n",
    "        self.extra_info = extra_info or {}\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 3: T√âL√âCHARGEMENT ET PR√âPARATION DU MOD√àLE GPT-2\n",
    "# ===============================================================\n",
    "\n",
    "# T√©l√©chargeons le mod√®le GPT-2 Medium et le tokenizer\n",
    "def download_and_save_model():\n",
    "    model_dir = \"/kaggle/working/gpt2-medium\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # T√©l√©charger le tokenizer et le mod√®le\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "    # D√©finir le token de padding pour GPT-2\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "    # Mettre √† jour la configuration du mod√®le pour reconna√Ætre le pad_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    # Sauvegarder le mod√®le et le tokenizer\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    tokenizer.save_pretrained(model_dir)\n",
    "    model.save_pretrained(model_dir)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Afficher un message de confirmation\n",
    "    print(f\"Temps √©coul√©: {elapsed_time:.2f} secondes\")\n",
    "    \n",
    "    return tokenizer, model\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 4: PR√âPARATION DES DONN√âES POUR L'ENTRA√éNEMENT\n",
    "# ===============================================================\n",
    "\n",
    "# Classe de dataset personnalis√©e\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", \n",
    "                                  max_length=max_length, return_tensors=\"pt\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "# Fonction pour diviser les textes en chunks\n",
    "def chunk_text(text, chunk_size_limit=600, overlap=20):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size_limit - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size_limit])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 5: CONSTRUCTION DE L'INDEX POUR LA RECHERCHE S√âMANTIQUE\n",
    "# ===============================================================\n",
    "\n",
    "# Construction de l'index\n",
    "def construct_index(directory_path):\n",
    "    # V√©rifier le GPU\n",
    "    device = check_gpu()\n",
    "    \n",
    "    # Param√®tres\n",
    "    max_input_size = 512  # Limit√© pour GPT-2\n",
    "    num_outputs = 256\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 600\n",
    "    \n",
    "    # T√©l√©charger et sauvegarder le mod√®le\n",
    "    tokenizer, model = download_and_save_model()\n",
    "    model.to(device)\n",
    "    \n",
    "    # V√©rifier si des documents existent dans le r√©pertoire\n",
    "    document_files = glob.glob(os.path.join(directory_path, \"*.txt\"))\n",
    "    if not document_files:\n",
    "        print(f\"Aucun fichier .txt trouv√© dans {directory_path}. Cr√©ation d'un exemple simple...\")\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    \n",
    "    if not documents:\n",
    "        raise ValueError(f\"Aucun document n'a √©t√© charg√© depuis {directory_path}\")\n",
    "    \n",
    "    print(f\"{len(documents)} document(s) charg√©(s)\")\n",
    "    \n",
    "    # Pr√©traiter les documents\n",
    "    all_chunks = []\n",
    "    document_embeddings = []\n",
    "    print(f\"Cr√©ation des chunks et des embeddings...\")\n",
    "\n",
    "    \n",
    "    # Barre de progression pour les chunks afin de suivre le traitement\n",
    "    for doc_idx, doc in enumerate(tqdm(documents, desc=\"Documents\", position=0)):\n",
    "        chunks = chunk_text(doc.text, chunk_size_limit, max_chunk_overlap)\n",
    "        all_chunks.extend(chunks)\n",
    "        \n",
    "        # Cr√©er des embeddings simples avec le mod√®le GPT-2\n",
    "        for chunk_idx, chunk in enumerate(tqdm(chunks, desc=f\"Embeddings pour doc {doc_idx+1}/{len(documents)}\", position=1, leave=False)):\n",
    "            inputs = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_size).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_hidden_states=True)\n",
    "                # Utiliser la moyenne de la derni√®re couche cach√©e comme embedding\n",
    "                last_hidden_state = outputs.hidden_states[-1].mean(dim=1)\n",
    "                document_embeddings.append(last_hidden_state.squeeze().cpu().numpy())\n",
    "    \n",
    "    # Convertir en array numpy pour faciliter la r√©cup√©ration\n",
    "    document_embeddings = np.array(document_embeddings)\n",
    "    \n",
    "    # Sauvegarder les donn√©es n√©cessaires\n",
    "    index_data = {\n",
    "        \"chunks\": all_chunks,\n",
    "        \"embeddings\": document_embeddings,\n",
    "    }\n",
    "    \n",
    "    torch.save(index_data, \"/kaggle/working/index.pt\")\n",
    "    \n",
    "    print(f\"Index cr√©√© et sauvegard√© dans /kaggle/working/index.pt\")\n",
    "    \n",
    "    return index_data\n",
    "\n",
    "# ===============================================================\n",
    "# SECTION 6: FINE-TUNING DU MOD√àLE SUR LES DOCUMENTS\n",
    "# ===============================================================\n",
    "\n",
    "# Fonction pour fine-tuner le mod√®le sur les documents\n",
    "def fine_tune_model(directory_path, epochs=3, batch_size=4, learning_rate=5e-5):\n",
    "    # V√©rifier le GPU\n",
    "    device = check_gpu()\n",
    "    \n",
    "    # T√©l√©charger le mod√®le\n",
    "    tokenizer, model = download_and_save_model()\n",
    "    \n",
    "    # Charger les documents\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    texts = [doc.text for doc in documents]\n",
    "    \n",
    "    if not texts:\n",
    "        raise ValueError(f\"Aucun document trouv√© pour le fine-tuning dans {directory_path}\")\n",
    "    \n",
    "    print(f\"{len(texts)} texte(s) charg√©(s) pour l'entra√Ænement\")\n",
    "    \n",
    "    # Cr√©er le dataset\n",
    "    dataset = TextDataset(texts, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Configurer l'optimiseur\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # D√©placer le mod√®le sur GPU \n",
    "    model.to(device)\n",
    "    \n",
    "    # Boucle d'entra√Ænement\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Initialiser le meilleur loss pour sauvegarder le meilleur mod√®le\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nüîÑ Epoch {epoch+1}/{epochs}\")\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Training\", position=0, \n",
    "                           bar_format='{l_bar}{bar:30}{r_bar}{bar:-30b}')\n",
    "        \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            # Obtenir les entr√©es et les envoyer sur GPU\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # R√©initialiser les gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Calculer les m√©triques et mettre √† jour la barre de progression\n",
    "            avg_loss = total_loss / (step + 1)\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            time_per_step = elapsed / (step + 1)\n",
    "            remaining_steps = len(dataloader) - (step + 1)\n",
    "            eta = time_per_step * remaining_steps\n",
    "            \n",
    "            # Mettre √† jour la barre de progression avec des informations d√©taill√©es\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{avg_loss:.4f}',\n",
    "                'elapsed': f'{int(elapsed//60)}m {int(elapsed%60)}s',\n",
    "                'ETA': f'{int(eta//60)}m {int(eta%60)}s',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "            \n",
    "            # Lib√©rer la m√©moire\n",
    "            if step % 10 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculer la perte moyenne pour cette √©poque\n",
    "        epoch_avg_loss = total_loss / len(dataloader)\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} termin√©e: Loss = {epoch_avg_loss:.4f}, Temps = {int(epoch_time//60)}m {int(epoch_time%60)}s\")\n",
    "        \n",
    "        # Sauvegarder le meilleur mod√®le\n",
    "        if epoch_avg_loss < best_loss:\n",
    "            best_loss = epoch_avg_loss\n",
    "            best_model_dir = \"/kaggle/working/gpt2-medium-fine-tuned-best\"\n",
    "            os.makedirs(best_model_dir, exist_ok=True)\n",
    "            model.save_pretrained(best_model_dir)\n",
    "            tokenizer.save_pretrained(best_model_dir)\n",
    "            print(f\"Meilleur mod√®le sauvegard√© (loss: {best_loss:.4f})\")\n",
    "    \n",
    "    # Sauvegarder le mod√®le fine-tun√© final\n",
    "    fine_tuned_dir = \"/kaggle/working/gpt2-medium-fine-tuned\"\n",
    "    os.makedirs(fine_tuned_dir, exist_ok=True)\n",
    "    model.save_pretrained(fine_tuned_dir)\n",
    "    tokenizer.save_pretrained(fine_tuned_dir)\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "    <div style=\"padding: 10px; border-radius: 5px; background-color: #eafaf1; border-left: 5px solid #2ecc71;\">\n",
    "      <h3 style=\"margin: 0;\">‚úÖ Fine-tuning termin√© avec succ√®s!</h3>\n",
    "      <p>Mod√®le final sauvegard√© dans <code>{fine_tuned_dir}</code></p>\n",
    "      <p>Meilleur mod√®le sauvegard√© dans <code>/kaggle/working/gpt2-medium-fine-tuned-best</code> (loss: {best_loss:.4f})</p>\n",
    "    </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cr√©ation de la fonction pour poser les questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T00:50:04.085851Z",
     "iopub.status.busy": "2025-04-24T00:50:04.085198Z",
     "iopub.status.idle": "2025-04-24T00:50:04.095543Z",
     "shell.execute_reply": "2025-04-24T00:50:04.094715Z",
     "shell.execute_reply.started": "2025-04-24T00:50:04.085826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fonction pour poser des questions\n",
    "def ask_me_anything(question, use_fine_tuned=True):\n",
    "    # Charger le mod√®le appropri√©\n",
    "    if use_fine_tuned and os.path.exists(\"/kaggle/input/base-de-reddit/modele-pre-entraine/modele-pre-entraine\"):\n",
    "        model_path = \"/kaggle/input/base-de-reddit/modele-pre-entraine/modele-pre-entraine\"\n",
    "    else:\n",
    "        model_path = \"/kaggle/working/gpt2-medium\"\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    # D√©finir le token de padding\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    \n",
    "    # Charger l'index\n",
    "    index_data = torch.load(\"/kaggle/input/base-de-reddit/index.pt\")\n",
    "    chunks = index_data[\"chunks\"]\n",
    "    embeddings = index_data[\"embeddings\"]\n",
    "    \n",
    "    # Cr√©er un embedding pour la question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        question_embedding = outputs.hidden_states[-1].mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    # Calculer les similarit√©s avec les chunks\n",
    "    similarities = cosine_similarity([question_embedding], embeddings)[0]\n",
    "    \n",
    "    # Trouver les chunks les plus pertinents\n",
    "    top_idx = np.argsort(similarities)[-3:][::-1]  # Top 3 chunks les plus similaires\n",
    "    \n",
    "    # CORRECTION: Limiter la taille du contexte pour ne pas d√©passer max_length\n",
    "    context_chunks = [chunks[i] for i in top_idx]\n",
    "    \n",
    "    # Choisir un contexte plus court si n√©cessaire\n",
    "    prompt = f\"Question: {question}\\nContexte: {context_chunks[0]}\\nR√©ponse:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Si le prompt est encore trop long, utiliser uniquement la question\n",
    "    if inputs[\"input_ids\"].shape[1] > 400:  # Laisse de la marge pour la g√©n√©ration\n",
    "        prompt = f\"Question: {question}\\nR√©ponse:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    response = response.split(\"R√©ponse:\")[-1].strip()\n",
    "    \n",
    "    display(Markdown(f\"You asked: <b>{question}</b>\"))\n",
    "    display(Markdown(f\"Bot says: <b>{response}</b>\"))\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du mod√®le sur nos donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-23T22:32:22.857846Z",
     "iopub.status.busy": "2025-04-23T22:32:22.857592Z",
     "iopub.status.idle": "2025-04-23T22:33:45.991167Z",
     "shell.execute_reply": "2025-04-23T22:33:45.990556Z",
     "shell.execute_reply.started": "2025-04-23T22:32:22.857818Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: Tesla T4 (15.83 GB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27e3494a8b84ad681d67d6b65f9882d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d967a729b145a9b33b9311b8b15f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40b1cdde7204750ad3f6d0ee4adb886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6811089fed4a70b1346f498c69dc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad01e1ead1d540a39eba4c9bcb797439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093058ac7d9847ec9603a1be9dc57273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a066b4f2344f8f9b1f2517368ec505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps √©coul√©: 10.85 secondes\n",
      "1 texte(s) charg√©(s) pour l'entra√Ænement\n",
      "\n",
      "üîÑ Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7e32c4a5ed4ecab4ee65f88ce39815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 termin√©e: Loss = 3.7245, Temps = 0m 1s\n",
      "Meilleur mod√®le sauvegard√© (loss: 3.7245)\n",
      "\n",
      "üîÑ Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b84ea79ff9430cb292152d94f20fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 termin√©e: Loss = 3.2840, Temps = 0m 0s\n",
      "Meilleur mod√®le sauvegard√© (loss: 3.2840)\n",
      "\n",
      "üîÑ Epoch 3/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9a9b9a6a7a4fb89e31a0f8a3d8633a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                              | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 termin√©e: Loss = 2.9781, Temps = 0m 0s\n",
      "Meilleur mod√®le sauvegard√© (loss: 2.9781)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"padding: 10px; border-radius: 5px; background-color: #eafaf1; border-left: 5px solid #2ecc71;\">\n",
       "      <h3 style=\"margin: 0;\">‚úÖ Fine-tuning termin√© avec succ√®s!</h3>\n",
       "      <p>Mod√®le final sauvegard√© dans <code>/kaggle/working/gpt2-medium-fine-tuned</code></p>\n",
       "      <p>Meilleur mod√®le sauvegard√© dans <code>/kaggle/working/gpt2-medium-fine-tuned-best</code> (loss: 2.9781)</p>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Construire l'index\n",
    "#index_data = construct_index(\"/kaggle/input/base-de-reddit/\")\n",
    "\n",
    "# 2. Fine-tuner le mod√®le\n",
    "#model, tokenizer = fine_tune_model(\"/kaggle/input/base-de-reddit\", epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester le chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T00:50:11.875871Z",
     "iopub.status.busy": "2025-04-24T00:50:11.875615Z",
     "iopub.status.idle": "2025-04-24T00:50:29.454455Z",
     "shell.execute_reply": "2025-04-24T00:50:29.453750Z",
     "shell.execute_reply.started": "2025-04-24T00:50:11.875856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/1451110233.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  index_data = torch.load(\"/kaggle/input/base-de-reddit/index.pt\")\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You asked: <b>What do people think about AI ?</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Bot says: <b>We are getting to the point where AI is not only going to be pervasive, it's going to be dominant. I would argue that by 2030, AI will have become a dominant force in our economy.\n",
       "AI will be the norm and you'll see it everywhere. You'll see people doing things in their home, your car, your home office, your job, your home office. You'll see it everywhere.\n",
       "AI is going to be ubiquitous and its impact will be felt across all sectors of our economy.\n",
       "And by 2030, you'll see AI becoming more and more of a driver of economic growth.\n",
       "The point is that AI is going to be a dominant force in the future.\n",
       "AI is going to be ubiquitous and its impact will be felt across all sectors of our economy.\n",
       "If AI is going to be pervasive, it's going to be dominant in all sectors.\n",
       "If AI is going to be dominant in all sectors, it's going to be dominant in every</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"We are getting to the point where AI is not only going to be pervasive, it's going to be dominant. I would argue that by 2030, AI will have become a dominant force in our economy.\\nAI will be the norm and you'll see it everywhere. You'll see people doing things in their home, your car, your home office, your job, your home office. You'll see it everywhere.\\nAI is going to be ubiquitous and its impact will be felt across all sectors of our economy.\\nAnd by 2030, you'll see AI becoming more and more of a driver of economic growth.\\nThe point is that AI is going to be a dominant force in the future.\\nAI is going to be ubiquitous and its impact will be felt across all sectors of our economy.\\nIf AI is going to be pervasive, it's going to be dominant in all sectors.\\nIf AI is going to be dominant in all sectors, it's going to be dominant in every\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_me_anything(\"What do people think about AI ?\", use_fine_tuned=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T22:46:45.305798Z",
     "iopub.status.busy": "2025-04-23T22:46:45.305113Z",
     "iopub.status.idle": "2025-04-23T22:47:58.559302Z",
     "shell.execute_reply": "2025-04-23T22:47:58.558357Z",
     "shell.execute_reply.started": "2025-04-23T22:46:45.305770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/ (stored 0%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/vocab.json (deflated 68%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/special_tokens_map.json (deflated 74%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/config.json (deflated 53%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/generation_config.json (deflated 24%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/merges.txt (deflated 53%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/model.safetensors (deflated 7%)\n",
      "  adding: kaggle/working/gpt2-medium-fine-tuned-best/tokenizer_config.json (deflated 56%)\n"
     ]
    }
   ],
   "source": [
    "#!zip -r /kaggle/working/gpt2-medium-fine-tuned-best.zip /kaggle/working/gpt2-medium-fine-tuned-best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cr√©ation de l'interface gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T23:34:43.895434Z",
     "iopub.status.busy": "2025-04-23T23:34:43.895153Z",
     "iopub.status.idle": "2025-04-23T23:34:48.917229Z",
     "shell.execute_reply": "2025-04-23T23:34:48.916504Z",
     "shell.execute_reply.started": "2025-04-23T23:34:43.895414Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3760614080.py:31: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=300)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "* Running on public URL: https://e238daadaf84a1a7e8.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e238daadaf84a1a7e8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# D√©finir la variable device si elle n'est pas d√©j√† d√©finie\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Fonction pour interagir avec le chatbot via Gradio\n",
    "def gradio_ask(question, history):\n",
    "    # Chargement des mod√®les et des donn√©es\n",
    "    if not os.path.exists(\"/kaggle/working/index.pt\"):\n",
    "        return \"Erreur: Veuillez d'abord ex√©cuter construct_index() pour cr√©er l'index.\", history\n",
    "    \n",
    "    # Vous pouvez modifier ce param√®tre pour utiliser le mod√®le fine-tun√© ou non\n",
    "    use_fine_tuned = os.path.exists(\"/kaggle/working/gpt2-medium-fine-tuned\")\n",
    "    \n",
    "    # Utiliser la fonction existante\n",
    "    response = ask_me_anything(question, use_fine_tuned=use_fine_tuned)\n",
    "    \n",
    "    # Formater pour Gradio - histoire normale, pas de type 'messages'\n",
    "    history.append((question, response))\n",
    "    return \"\", history\n",
    "\n",
    "# Cr√©ation de l'interface Gradio\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks(title=\"Teknoloogiaa\") as demo:\n",
    "        gr.Markdown(\"# Teknoloogiaa\")\n",
    "        gr.Markdown(\"Posez vos questions a Teknoloogiaa sur tout ce qui est en rapport avec l'IA, le machine learning, la datascience, ...\")\n",
    "        \n",
    "        # Retirez le param√®tre type='messages'\n",
    "        chatbot = gr.Chatbot(height=300)\n",
    "        \n",
    "        # Cr√©ation d'une ligne avec un champ texte et un bouton d'envoi\n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(placeholder=\"Posez votre question ici...\", lines=1, scale=4)\n",
    "            submit_btn = gr.Button(\"Envoyer\", scale=1)\n",
    "        \n",
    "        clear = gr.Button(\"Effacer la conversation\")\n",
    "        \n",
    "        # Connecter le bouton d'envoi √† la fonction gradio_ask\n",
    "        submit_btn.click(gradio_ask, [msg, chatbot], [msg, chatbot])\n",
    "        \n",
    "        # Garder la possibilit√© d'envoyer en appuyant sur Entr√©e\n",
    "        msg.submit(gradio_ask, [msg, chatbot], [msg, chatbot])\n",
    "        \n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "        \n",
    "        \n",
    "    return demo\n",
    "\n",
    "# Pour lancer l'interface:\n",
    "demo = create_gradio_interface()\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7225506,
     "sourceId": 11536596,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
